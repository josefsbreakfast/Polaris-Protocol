# ğŸ“œ Incomplete Search Behaviour (Diagnostics)  
**First created:** 2025-11-18 | **Last updated:** 2025-11-18  
*How to identify, evidence, and frame the systematic signs that an institution has performed a partial, silo-limited, or flawed search in response to FOI or SAR obligations.*

---

## ğŸ§­ Orientation  
Most institutions do **not** intentionally withhold data.  
They simply:

- search the wrong system,  
- search only a single mailbox,  
- rely on the memory of an under-briefed manager,  
- forget the vendor chain,  
- exclude cloud logs,  
- interpret your request too narrowly,  
- miss metadata holders entirely.

Incomplete searches generate the contradictions, gaps, and anomalies that later feed into:

- **ICO escalation**,  
- **vendor mapping**,  
- **institutional panic**,  
- **triangulated FOI/SAR logic**.

This node maps the diagnostic signs that a search was *not* complete â€”  
even if they assert it was.

---

# ğŸ§© The Nine Diagnostically Reliable Signs of an Incomplete Search  

These are the indicators you use when constructing an ICO-ready evidence packet.

---

## **1. Narrow Interpretation of the Request**  
The response focuses on:

- one team,  
- one mailbox,  
- one system,  
- one date window,  
- one keyword.

Language markers include:

- â€œWe searched the X teamâ€™s filesâ€¦â€  
- â€œWe checked our main systemâ€¦â€  
- â€œWe reviewed the email account ofâ€¦â€  

This is not a full search.  
It is a *subset* search disguised as a full one.

---

## **2. Absence of Metadata in the Response**  
If the SAR does *not* include:

- system identifiers,  
- timestamps,  
- environment tags,  
- processor codes,  
- audit logs,

â€¦it means they did not query the underlying systems.

This is the clearest sign an institution only searched *documents*,  
not *data processing pathways*.

---

## **3. FOI and SAR Describe Different Data Universes**  
Example:

- FOI: â€œWe do not use automated profiling.â€  
- SAR: contains **risk scores**, **classifiers**, or **automated summary fields**.

This is a textbook incomplete search in at least one direction.

---

## **4. Missing Internal Correspondence**  
A genuinely full search should include:

- internal emails,  
- minutes,  
- Slack/Teams logs (where applicable),  
- decisions,  
- instructions,  
- searches performed.

If these are absent,  
you know they didnâ€™t search organisational communications.

---

## **5. â€œWe Found Nothingâ€ Replies for High-Activity Periods**  
If the period in question includes:

- a complaint,  
- a process,  
- a review,  
- a risk event,  
- institutional contact,  
- public or legal activity,

and **no internal correspondence exists**,  
the search was incomplete.

Bodies never produce *zero* internal traffic during crises.

---

## **6. Absence of Vendor Records**  
If the organisation uses:

- cloud systems,  
- analytics tools,  
- behaviour-scoring engines,  
- ticketing systems,

and none appear in the SAR or FOI response,  
this is a **vendor blindspot** (Node 26)  
and indicates an incomplete search.

---

## **7. Discrepant Timestamps**  
If:

- the SARâ€™s timestamps do not match their stated processing practices,  
- processing happened outside office hours,  
- logs appear in batch cycles,  
- there are timezone mismatches,

â€¦it means another system was involved that they did not search.

---

## **8. Abruptly Short Responses**  
Any response under ~2â€“4 pages for a complex case is suspicious.

Institutions generate:

- drafts,  
- queries,  
- escalations,  
- clarifications,  
- internal instructions.

If none of these appear,  
the search was almost certainly limited.

---

## **9. â€œWe Searched What You Asked Forâ€ (But They Didnâ€™t Ask Clarifying Questions)**  
A key diagnostic marker:

If a request is broad or ambiguous,  
the institution has a duty to seek clarification.

If they do *not* seek clarification  
and produce a very narrow answer,  
that is a predictable incomplete search.

---

# ğŸ§¨ How Incomplete Search Produces Systemic Contradictions  

Incomplete searches surface as:

### âœ” Eventâ€“Record Inconsistency  
e.g.,  
You have lived experience of an event â†’  
They have zero internal records of it.

### âœ” FOIâ€“SAR Mismatch  
One response names a system the other doesnâ€™t.

### âœ” Timeline Drift  
Gaps around critical dates.

### âœ” Metadata Absence  
Signals they did not search the systems that actually processed your data.

### âœ” Vendor Absence  
Signals they did not check third-party or cloud environments.

These contradictions become **ICO-grade evidence**.

---

# ğŸ”§ How to Present Incomplete Search Without Accusation  

Use gentle phrasing such as:

- â€œThis may indicate the search was not sufficiently wide.â€  
- â€œThere appear to be discrepancies between your systemsâ€™ metadata and the search results.â€  
- â€œCould you confirm whether cloud environments or vendor-held data were included in the search?â€  
- â€œThere may be additional systems that were not captured.â€

Each line signals the problem  
without provoking institutional defensiveness.

---

# ğŸ§  Key Insight  
> **Incomplete search behaviour is not malicious â€”  
> it is structural.  
>  
> But it reveals where the truth is hiding.**

In Polaris, â€œincomplete searchâ€ is the first layer of forensic illumination.

It shows which systems exist,  
which vendors are involved,  
which silos were bypassed,  
and where internal contradictions originate.

---

# ğŸŒŒ Constellations  
Transparency_Warfare Â· Metadata_Foreensics Â· Institutional_Drift Â· FOI_Strategy Â· Data_Integrity  

---

# ğŸ® Footer  
This node supports:

- **Triangulated FOI/SAR Method**,  
- **Pre-Escalation Friction Mapping**,  
- **ICO-Ready Contradiction Framing**,  
- **Vendor Blindspot Mapping**,  
- **Institutional Panic Dynamics**,  
- **Seven Layers of Safeguard Breakdown**.

Together these nodes give Polaris users the complete diagnostic toolkit  
for identifying hidden systems and demanding lawful transparency.
