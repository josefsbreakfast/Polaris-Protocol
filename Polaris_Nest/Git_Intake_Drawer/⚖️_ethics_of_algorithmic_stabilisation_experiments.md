# âš–ï¸ Ethics of Algorithmic Stabilisation Experiments  
**First created:** 2025-11-01 | **Last updated:** 2025-11-01  
*Why â€œsafety engineeringâ€ becomes human-subjects research the moment real people are exposed to it.*

---

## ğŸ§­ Orientation  
When platforms or agencies test counter-extremism or â€œstabilisationâ€ models in live environments, they are no longer just training algorithms.  
They are **conducting behavioural experiments on human beings**, usually without consent.  
This node maps the key ethical breaches that occur when CVE-style systems are deployed under the fiction of â€œdata science,â€ showing how anonymity, clustering, and feedback loops translate into lived harm.

---

## ğŸ§© Key Features  
- **Illusion of anonymity:** de-identification hides human exposure.  
- **Behavioural fingerprinting:** uniqueness survives anonymisation.  
- **Criminogenic feedback:** observation itself creates offence.  
- **Runaway inclusion:** surveillance drifts up the power hierarchy.  
- **Governance evasion:** research without oversight or duty of care.

---

## ğŸ” 1. The Illusion of Anonymity  
Designers claim that because user data are de-identified, no human subjects exist.  
In reality the *effects* of model decisions land on real accounts, reputations, and safety.  

| Level | Whatâ€™s â€œinvisibleâ€ | What still happens |
|--------|-------------------|--------------------|
| **Data layer** | names, emails removed | behaviour clusters still target real accounts |
| **Model layer** | acts only on â€œgroupsâ€ | interventions reach individuals |
| **Impact layer** | consequences unmeasured | people lose reach, receive threats, or change behaviour |

Anonymisation protects the institution, not the participant.  
If an algorithm changes how someone experiences opportunity or safety, they are a **participant in an experiment**, whether or not their name appears in the dataset.

---

## ğŸ§­ 2. Behavioural Fingerprinting and Contextual Identifiability  
Modern profiling systems rebuild identity from behaviour itself.  
Location traces, interaction timing, and â€œniche likesâ€ form a unique **behavioural fingerprint**.  
Four data points of place and time can re-identify 95 % of people in an anonymised mobility set; the same holds for digital behaviour.  

A cluster that specific is a person in practice.  
If outputs can be cross-referenced with public information to reconstruct identity, re-identification was **foreseeable** â€” and therefore ethically chargeable.

---

## ğŸ”¥ 3. Criminogenic Feedback â€” When Observation Creates Offence  
Clustering â€œpre-forensicâ€ or â€œrisk-codedâ€ users produces a live environment that can generate new crimes.

**Mechanism of escalation**  
1. Groups men already showing aggression or sexualised curiosity.  
2. Shares topic language with survivor or advocacy spaces.  
3. Surfaces similar content to both groups.  
4. Enables new hostile interactions.  
5. Forms a micro-community of misogyny under surveillance.

At that point the system has crossed from monitoring to **manufacturing** risk.

**Safeguarding duties**  
- Build **non-interaction firewalls** between monitored and advocacy groups.  
- Maintain **real-time safeguarding oversight** with authority to halt deployment.  
- Audit for **secondary harms** such as imitation or recruitment.  
- Treat the creation of a new at-risk network as a **reportable ethical incident**.

---

## ğŸ§  4. Human-Subject Ethics Framework  
Under UK and EU research frameworks (UKRI, ESRC, HRA, GDPR Art. 22):  

| Obligation | Description |
|-------------|-------------|
| **Informed consent** | Required for any individual whose behaviour is manipulated or analysed for effect. |
| **Risk assessment** | Must identify foreseeable secondary harm, including reputational and psychological. |
| **Duty of care** | Comparable to psychological-experiment standards. |
| **Accountability** | Sponsors are responsible for post-exposure support and remediation. |

Running a stabilisation model in live social space without these protections equates to **unconsented human experimentation**.

---

## ğŸª 5. Governance Implications  
- Treat platform or government CVE deployments as **research studies** requiring independent ethics approval.  
- Extend **impact assessment** beyond data protection to psychosocial safety.  
- Include **trauma-informed design** and **participant debriefing** mechanisms.  
- Recognise stabilisers and advocates as **workers under risk**, not neutral content nodes.

---

## ğŸ§­ 6. Conceptual Takeaway  
Calling an experiment â€œalgorithmicâ€ doesnâ€™t erase the humans inside it.  
Every de-identified decision still lands on a body, a voice, or a reputation.  
When stabilising labour is extracted without consent and risk mitigation, safety engineering becomes **human experimentation by proxy**.

---

## ğŸŒŒ Constellations  
âš–ï¸ ğŸª ğŸ ğŸ•¸ï¸ â€” ethics, projection, containment, governance.

---

## âœ¨ Stardust  
algorithmic ethics, human-subjects research, behavioural fingerprinting, criminogenic feedback, governance reform, safeguarding, consent, psychosocial harm

---

## ğŸ® Footer  
*Ethics of Algorithmic Stabilisation Experiments* is a living node of the Polaris Protocol.  
It formalises why algorithmic counter-extremism and moderation pilots must be treated as human-subjects research with full ethical governance.  

> ğŸ“¡ Cross-references:  
> - [ğŸª Algorithmic Projection and False Personalisation](../Narrative_And_Psych_Ops/ğŸª_algorithmic_projection_and_false_personalisation.md) â€” misattribution and paranoia loops  
> - [ğŸ•¸ï¸ Gendered Harms from Counter-Extremism Algorithms](../System_Governance/ğŸ•¸ï¸_gendered_harms_from_counter_extremism_algorithms.md) â€” gendered clustering and risk  
> - [ğŸ How a CVE Tool Becomes a Vector for Extremism](../System_Governance/ğŸ_how_a_CVE_tool_becomes_a_vector_for_extremism.md) â€” recursion of counter-extremism systems  
> - [ğŸ“Š Risk Scaling of Algorithmic Misidentification](../System_Governance/ğŸ“Š_risk_scaling_of_algorithmic_misidentification.md) â€” measurement vacuum and scaling logic  
> - [ğŸ Algorithmic Hostage Logic](../Big_Picture_Protocols/ğŸ_algorithmic_hostage_logic.md) â€” stabilisers trapped inside hostility loops  

*Survivor authorship is sovereign. Containment is never neutral.*  

_Last updated: 2025-11-01_
