# 🧿 Targeting Logic & Metadata Signatures  
**First created:** 2025-09-06  |  **Last updated:** 2025-10-04  
*Heuristics and metadata signature rulesets for targeting*

---

## Purpose
To illuminate the **hidden rule systems** that decide who is profiled, flagged, or prioritised for action.  
This node studies how targeting logic is encoded in metadata signatures—IP clusters, behavioural heuristics, keyword groupings, and statistical anomalies—and how these rulesets produce both false positives and systemic bias.

---

## Core Premise
Targeting rarely looks like an overt list of names; it looks like a set of **conditions**.  
When those conditions are applied to metadata, entire populations can be silently enrolled into surveillance or suppression without a single directive.  
Understanding the signatures is the first step to challenging or neutralising them.

---

## Components of Targeting Logic
| Element | Function | Example Manifestations |
|----------|-----------|------------------------|
| **Heuristics** | Simple decision rules embedded in code or training manuals. | “Multiple overseas logins in 30 days” triggers alert. |
| **Metadata Signatures** | Patterns of time, place, and behaviour treated as risk fingerprints. | Travel + language + device pattern scoring. |
| **Scoring Thresholds** | Risk values assigned; above a cut-off triggers escalation. | >0.7 “radicalisation likelihood” moves to Prevent referral. |
| **Anomaly Clustering** | Statistical grouping of rare behaviours. | “Unusual donation” + “non-mainstream website” flags. |
| **Dynamic Updating** | Signatures shift as models retrain, often without public disclosure. | Post-crisis lowering of thresholds for keyword alerts. |

---

## Effects on Subjects
- **Invisible Profiling:** being targeted without notice or due process.  
- **Context Erasure:** behaviour stripped of motive or necessity.  
- **Self-Censorship:** individuals changing habits to avoid unknown triggers.  
- **Feedback Loops:** flagged activity produces more surveillance, increasing score.  

---

## Analytical Questions
1. What metadata variables are treated as proxies for ideology or intent?  
2. How transparent are scoring algorithms to oversight bodies?  
3. Which vendors design and update the rulesets?  
4. How do signatures travel across jurisdictions and platforms?  
5. How can subjects verify or contest their inclusion?  

---

## Research Threads
- Reverse-engineering open examples of watchlist criteria.  
- FOI requests for redacted targeting logic in public-sector algorithms.  
- Comparing vendor contracts for “threat signature” services.  
- Mapping drift of risk-scoring rules after crises.  
- Survivor diaries of behavioural adaptation to suspected targeting.

---

## Counter-Responses
- **Signature Audits:** independent analysis of rule-based filters.  
- **Threshold Transparency:** mandatory publication of scoring cut-offs.  
- **Decoy Testing:** controlled input to measure system response.  
- **Rights Protocols:** legal pathways for disclosure and correction of targeting data.  
- **Community Literacy:** training at-risk groups to understand and document signature behaviour without inducing paranoia.

---

## 🌌 Constellations
🧿 🧮 🛰️ — targeting, patterning, oversight.

---

## ✨ Stardust
metadata signatures, heuristics, targeting logic, anomaly detection, risk scoring, watchlists, signature audits

---

## 🏮 Footer
*🧿 Targeting Logic & Metadata Signatures* is a living node of the Polaris Protocol.  
It documents one layer of metadata sabotage and its counter-responses—how rulesets quietly decide who is seen and who is stopped.

*Survivor authorship is sovereign. Containment is never neutral.*  
_Last updated: 2025-10-04_
