# ðŸ¶ Internal Dogfooding as a Risk Vector  
**First created:** 2025-12-14 | **Last updated:** 2026-01-18  
*How internal testing practices create distorted behavioural models that later affect real users*

---

## ðŸ§­ What This Node Is About  

Internal dogfooding refers to the practice of employees using their own organisationâ€™s products, tools, or experimental systems during development and testing.

In principle, dogfooding is sensible.  
In practice, it introduces a **highly specific and under-governed risk** in behavioural modelling systems.

When data generated by internal users is used to train, calibrate, or structure models that later operate on the public, **internal behaviour becomes a silent reference population**.

This node documents how that happens â€” and why it matters.

---

## ðŸ§© What Makes Dogfooding Data Structurally Different  

Internal users are not representative users.

They are more likely to:

- understand system intent and internals  
- deliberately test edge cases  
- use features intensely or atypically  
- generate high-context, high-density data  
- behave strategically rather than naturally  
- share professional norms and workflows  
- cluster tightly in behavioural space  

As a result, dogfooding datasets are **dense, narrow, and skewed**.

They are useful for testing.  
They are dangerous as baselines.

---

## ðŸ” How Dogfooding Becomes a Modelling Input  

Dogfooding data becomes operational when it is used to:

- generate early embeddings  
- define cluster boundaries  
- calibrate anomaly detection  
- tune proxy thresholds  
- shape recommendation geometry  
- train safety or moderation heuristics  
- â€œbootstrapâ€ models before scale  

At this stage, the internal cohort is no longer just testing.  
It is **defining normality**.

---

## ðŸ§® Why Dogfooding Creates Micro-Clusters  

Because dogfooding populations are small and behaviourally similar, they often produce:

- extremely tight clusters  
- sharp boundaries  
- exaggerated similarity  
- strong centroids  
- minimal internal variance  

If only two or three internal users exhibit a certain behaviour pattern, the model may encode that pattern as a **distinct behavioural region**.

When external users later resemble that pattern, they are mapped into a space that was never designed for them.

---

## ðŸ‘» Interaction with Embedding Inertia  

Dogfooding artefacts persist because:

- early embeddings are reused  
- cluster geometry becomes foundational  
- later data is mapped onto existing space  
- retraining preserves initial structure  
- no explicit â€œdogfooding purgeâ€ occurs  

Even when the original internal users stop generating data, the **shape they created remains**.

This is how internal behaviour echoes outward long after the testing phase ends.

---

## ðŸ•³ï¸ Why This Failure Is Rarely Documented  

Dogfooding risks are often invisible because:

- internal testing is considered benign  
- datasets are assumed to be temporary  
- artefacts are labelled â€œtechnicalâ€  
- consent frameworks treat employees differently  
- model lineage is poorly tracked  
- success metrics ignore representativeness  

As a result, dogfooding influence is rarely disclosed â€” even internally.

---

## âš–ï¸ Interaction with Consent and Power Asymmetry  

Dogfooding raises specific consent concerns:

- employees may not meaningfully consent to downstream reuse  
- power dynamics complicate refusal  
- data may be repurposed beyond original scope  
- experimental inference may affect non-employees later  

When dogfooding data shapes public-facing systems, **employee data indirectly governs others**.

This is a non-obvious but serious ethical issue.

---

## âš ï¸ When Dogfooding Becomes Harmful  

Dogfooding becomes harmful when:

- internal behaviour defines behavioural â€œtypesâ€  
- sparse internal clusters persist in production  
- proxy misinterpretation is calibrated on employees  
- external users are assessed against internal norms  
- misclassification feels personal or uncanny  
- individuals have no visibility into the source of inference  

At this point, the system is no longer testing itself.  
It is projecting itself onto others.

---

## ðŸ› ï¸ What Responsible Containment Requires  

Responsible handling of dogfooding requires:

- explicit labelling of internal datasets  
- hard separation from production training  
- mandatory retraining on representative data  
- removal of internal embeddings before deployment  
- consent clarity for employee data  
- audit trails for artefact provenance  
- sunset clauses on dogfooding influence  

Without these measures, dogfooding becomes a silent design authority.

---

## ðŸŽ¯ Key Takeaway  

> **Dogfooding does not just test systems.  
It teaches systems what â€œnormalâ€ looks like.**

If that lesson is not unlearned before deployment, everyone else inherits it.

---

## ðŸŒŒ Constellations  
ðŸ¶ ðŸ§ª ðŸ‘» ðŸ§® âš–ï¸ â€” internal bias, experimental spillover, legacy influence, sparsity amplification, governance risk

---

## âœ¨ Stardust  
internal dogfooding, employee data, behavioural baselines, testing bias, representativeness failure, prototype risk, consent asymmetry

---

## ðŸ® Footer  

*ðŸ¶ Internal Dogfooding as a Risk Vector* is a living node of the **Polaris Protocol**, documenting how internal testing practices quietly shape behavioural models and why this influence becomes harmful when left unexamined in production systems.

This node frames dogfooding not as misuse, but as a **powerful and under-governed modelling input**.

> ðŸ“¡ Cross-references:
> 
> - [ðŸ§ª R&D Artefact Leakage into Production](./ðŸ§ª_rnd_artefact_leakage_into_production.md) â€” *how experimental outputs escape containment*  
> - [ðŸ‘» Embedding Inertia and Ghost Geometry](../ðŸ‘¾_Breakpoints_And_Glitches/ðŸ‘»_embedding_inertia_and_ghost_geometry.md) â€” *why internal influence persists*  
> - [ðŸ§® Sparse Cluster Pathology (Micro-Clusters)](./ðŸ§®_sparse_cluster_pathology_microclusters.md) â€” *how small populations distort meaning*  

*Survivor authorship is sovereign. Containment is never neutral.*

_Last updated: 2026-01-18_
