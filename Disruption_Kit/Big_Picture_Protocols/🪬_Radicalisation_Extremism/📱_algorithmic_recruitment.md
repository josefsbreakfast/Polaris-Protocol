# 📱 Algorithmic Recruitment  
**First created:** 2025-09-12  |  **Last updated:** 2025-10-04  
*Analysis of TikTok, YouTube, and Telegram recommendation loops as accelerants of extremist worldview adoption*

---

## Purpose
To examine how content-personalisation systems function as **recruitment infrastructure**—guiding users from mainstream curiosity to ideological immersion through invisible patterning.  
This node investigates both platform logic and user affect: how algorithmic “care” becomes coercion, and how attention is trained to mistake repetition for truth.

---

## Core Premise
Radicalisation online is rarely an explicit invitation.  
It is a sequence of **micro-corrections** made by recommendation engines responding to engagement metrics.  
Each click tightens the spiral: aesthetic → argument → grievance → identity → action.

---

## Recruitment Loops
| Stage | Mechanism | Example Manifestations |
|--------|------------|------------------------|
| **1. Emotional Hook** | Algorithms favour emotionally charged content (fear, outrage, belonging). | “The truth they won’t tell you about…”; climate despair reels. |
| **2. Aesthetic Familiarity** | Visual/aural tropes create trust through recognisable rhythm or tone. | Military montages synced to trending audio. |
| **3. Ideological Drift** | Gradual exposure to extreme content via “adjacent recommendations.” | Fitness → nationalism → conspiracy. |
| **4. Community Capture** | Private groups reinforce new norms and punish dissent. | Telegram chats reframing loyalty as awakening. |
| **5. Platform Exit** | Transition from mainstream app to encrypted or federated platforms. | Migration from YouTube to Odysee or private Discord servers. |

---

## Data Points to Track
- **Entry Keywords:** what neutral searches begin the descent (e.g., “fitness,” “self-sufficiency,” “political compass”).  
- **Recommendation Chains:** sequence of suggested content leading to ideological pivot.  
- **Time-to-Radicalisation (TTR):** average watch or scroll hours between neutral and extremist material.  
- **Affective Markers:** shifts in tone, soundtrack, or imagery signalling moral urgency.  
- **Cross-Platform Drift:** how Telegram or Discord groups exploit platform bans to recruit.  

---

## Observed Patterns
- Algorithms reward **certainty over nuance**; ambiguity is deranked.  
- “Community policing” mechanisms often suppress de-radicalising content faster than hate material.  
- “Edutainment” creators act as unwitting on-ramps, producing content with identical emotional cadence to extremist propaganda.  
- Users describe the progression as *discovery*, not indoctrination—an illusion reinforced by algorithmic reinforcement of perceived autonomy.  

---

## Research Threads
- Reverse-engineering recommendation APIs under controlled conditions.  
- Mapping visual motifs and sound trends associated with extremist ascents.  
- Analysing engagement data post-ban to track audience migration.  
- Studying affect contagion in “reaction” content ecosystems.  
- Collaborating with survivor communities for testimony on online grooming narratives.

---

## Future Expansion
This node will form part of the **🪬 Radicalisation & Extremism** constellation alongside:
- **🌍 Climate Panic Routing** — eco-anxiety as recruitment vector.  
- **📣 Mainstream Visibility Gaps** — post-exposure silence as risk multiplier.  
- **🕊️ False Counter-Extremism Dialogues** — controlled opposition masquerading as prevention.

---

## 🌌 Constellations
📱 🪬 🧠 — algorithm, affect, ideology.

---

## ✨ Stardust
algorithmic radicalisation, recommendation loops, affect engineering, social platforms, recruitment pipelines, behavioural governance

---

## 🏮 Footer
Recruitment no longer knocks; it recommends.  
This node listens to the hum between the clicks.

*Survivor authorship is sovereign. Containment is never neutral.*  
_Last updated: 2025-10-04_
